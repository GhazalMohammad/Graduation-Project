{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e289a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import tweepy\n",
    "from textblob import TextBlob\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import nltk\n",
    "import pycountry\n",
    "import re\n",
    "import string\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from PIL import Image\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from langdetect import detect\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import time\n",
    "from IPython.display import display\n",
    "from functools import partial\n",
    "from nltk.corpus import stopwords\n",
    "import geopandas as gpd\n",
    "from geopandas.tools import geocode\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.exc import GeocoderTimedOut\n",
    "from datetime import datetime\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce1ca9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authentication\n",
    "consumerKey = 'kGGFOcIVBZRxExoct2nQwrlOI'\n",
    "consumerSecret = 'F66rwDS0dadWJzmuors8JGnez9q53JCpMER76nD6LCELDXkBLF'\n",
    "accessToken = '1579551912587927553-vGRMDNwQwXYC2JgBZ4sB2BBPaQUjWC'\n",
    "accessTokenSecret = 'ra1EJ5zdDZbF6sjD0rHzINb7ljKt4ioaDxRWjGwXJfMZH'\n",
    "auth = tweepy.OAuthHandler(consumerKey, consumerSecret)\n",
    "auth.set_access_token(accessToken, accessTokenSecret)\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48dcccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sentiment Analysis\n",
    "def percentage(part,whole):\n",
    " return 100 * float(part)/float(whole)\n",
    "keyword = input('Please enter keyword or hashtag to search :')\n",
    "noOfTweet = int(input('Please enter how many tweets to analyze :'))\n",
    "tweets = tweepy.Cursor(api.search_tweets, q=keyword).items(noOfTweet)\n",
    "positive = 0\n",
    "negative = 0\n",
    "neutral = 0\n",
    "polarity = 0\n",
    "tweet_list = []\n",
    "neutral_list = []\n",
    "negative_list = []\n",
    "positive_list = []\n",
    "for tweet in tweets:\n",
    "\n",
    "    # print(tweet.text)\n",
    "    tweet_list.append(tweet.text)\n",
    "    analysis = TextBlob(tweet.text)\n",
    "    score = SentimentIntensityAnalyzer().polarity_scores(tweet.text)\n",
    "    neg = score['neg']\n",
    "    neu = score['neu']\n",
    "    pos = score['pos']\n",
    "    comp = score['compound']\n",
    "    polarity += analysis.sentiment.polarity\n",
    "\n",
    "    if neg > pos:\n",
    "        negative_list.append(tweet.text)\n",
    "        negative += 1\n",
    "    elif pos > neg:\n",
    "        positive_list.append(tweet.text)\n",
    "        positive += 1\n",
    "    elif pos == neg:\n",
    "        neutral_list.append(tweet.text)\n",
    "        neutral += 1\n",
    "\n",
    "positive = percentage(positive, noOfTweet)\n",
    "negative = percentage(negative, noOfTweet)\n",
    "neutral = percentage(neutral, noOfTweet)\n",
    "polarity = percentage(polarity, noOfTweet)\n",
    "positive = format(positive, '.1f')\n",
    "negative = format(negative, '.1f')\n",
    "neutral = format(neutral, '.1f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e9ed89",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_list = pd.DataFrame(tweet_list)\n",
    "neutral_list = pd.DataFrame(neutral_list)\n",
    "negative_list = pd.DataFrame(negative_list)\n",
    "positive_list = pd.DataFrame(positive_list)\n",
    "print('total number:', len(tweet_list))\n",
    "print('positive number:', len(positive_list))\n",
    "print('negative number:', len(negative_list))\n",
    "print('neutral number: ', len(neutral_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0d737d",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['Positive ['+str(positive)+'%]', 'Neutral [' +\n",
    "          str(neutral)+'%]', 'Negative ['+str(negative)+'%]']\n",
    "sizes = [positive, neutral, negative]\n",
    "colors = ['yellowgreen', 'blue', 'red']\n",
    "patches, texts = plt.pie(sizes, colors=colors, startangle=90)\n",
    "plt.style.use('default')\n",
    "plt.legend(labels)\n",
    "plt.title(\"Sentiment Analysis Result for keyword= \"+keyword+\"\")\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f46f94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b831c715",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_list.drop_duplicates(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9539a46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning Text (RT, Punctuation etc)\n",
    "#Creating new dataframe and new features\n",
    "tw_list = pd.DataFrame(tweet_list)\n",
    "tw_list['text'] = tw_list[0]\n",
    "#Removing RT, Punctuation etc\n",
    "remove_rt = lambda x: re.sub('RT @\\w+: ',\" \",x)\n",
    "rt = lambda x: re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",x)\n",
    "\n",
    "tw_list['text'] = tw_list.text.map(remove_rt).map(rt)\n",
    "tw_list['text'] = tw_list.text.str.lower()\n",
    "\n",
    "tw_list.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b4aa66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating Negative, Positive, Neutral and Compound values\n",
    "# Calculating Negative, Positive, Neutral and Compound values\n",
    "tw_list[['polarity', 'subjectivity']] = tw_list['text'].apply(\n",
    "    lambda Text: pd.Series(TextBlob(Text).sentiment))\n",
    "\n",
    "for index, row in tw_list['text'].iteritems():\n",
    "    score = SentimentIntensityAnalyzer().polarity_scores(row)\n",
    "    neg = score['neg']\n",
    "    neu = score['neu']\n",
    "    pos = score['pos']\n",
    "    comp = score['compound']\n",
    "    if neg > pos:\n",
    "        tw_list.loc[index, 'sentiment'] = \"negative\"\n",
    "    elif pos > neg:\n",
    "        tw_list.loc[index, 'sentiment'] = \"positive\"\n",
    "    else:\n",
    "        tw_list.loc[index, 'sentiment'] = \"neutral\"\n",
    "    tw_list.loc[index, 'neg'] = neg\n",
    "    tw_list.loc[index, 'neu'] = neu\n",
    "    tw_list.loc[index, 'pos'] = pos\n",
    "    tw_list.loc[index, 'compound'] = comp\n",
    "\n",
    "tw_list.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2ea7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tw_list_negative = tw_list[tw_list[\"sentiment\"] == \"negative\"]\n",
    "tw_list_positive = tw_list[tw_list[\"sentiment\"] == \"positive\"]\n",
    "tw_list_neutral = tw_list[tw_list[\"sentiment\"] == \"neutral\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c105681",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_values_in_column(data,feature):\n",
    " total=data.loc[:,feature].value_counts(dropna=False)\n",
    " percentage=round(data.loc[:,feature].value_counts(dropna=False,normalize=True)*100,2)\n",
    " return pd.concat([total,percentage],axis=1,keys=['Total','Percentage'])\n",
    "#Count_values for sentiment\n",
    "count_values_in_column(tw_list,\"sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228bb8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data for Pie Chart\n",
    "pichart = count_values_in_column(tw_list, \"sentiment\")\n",
    "names = pichart.index\n",
    "size = pichart[\"Percentage\"]\n",
    " \n",
    "# Create a circle for the center of the plot\n",
    "my_circle = plt.Circle((0, 0), 0.7, color='white')\n",
    "plt.pie(size, labels=names, colors=['green', 'blue', 'red'])\n",
    "p = plt.gcf()\n",
    "p.gca().add_artist(my_circle)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f0ee9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Create Wordcloud\n",
    "\n",
    "\n",
    "def create_wordcloud(text):\n",
    "    mask = np.array(Image.open(\"cloud.png\"))\n",
    "    stopwords = set(STOPWORDS)\n",
    "    wc = WordCloud(background_color=\"white\",\n",
    "                   mask=mask,\n",
    "                   max_words=3000,\n",
    "                   stopwords=stopwords,\n",
    "                   repeat=True)\n",
    "    wc.generate(str(text))\n",
    "    wc.to_file(\"wc.png\")\n",
    "    print(\"Word Cloud Saved Successfully\")\n",
    "    path = \"wc.png\"\n",
    "    display(Image.open(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9777482",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating wordcloud for all tweets\n",
    "create_wordcloud(tw_list[\"text\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1506dc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating wordcloud for positive sentiment\n",
    "create_wordcloud(tw_list_positive[\"text\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ea6fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating wordcloud for negative sentiment\n",
    "create_wordcloud(tw_list_negative[\"text\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd22c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating tweet’s lenght and word count\n",
    "tw_list['text_len'] = tw_list['text'].astype(str).apply(len)\n",
    "tw_list['text_word_count'] = tw_list['text'].apply(lambda x: len(str(x).split()))\n",
    "round(pd.DataFrame(tw_list.groupby(\"sentiment\").text_len.mean()),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207c6e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "round(pd.DataFrame(tw_list.groupby(\"sentiment\").text_word_count.mean()),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a05e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing Punctuation\n",
    "def remove_punct(text):\n",
    " text = \"\".join([char for char in text if char not in string.punctuation])\n",
    " text = re.sub('[0–9]+', '', text)\n",
    " return text\n",
    "tw_list['punct'] = tw_list['text'].apply(lambda x: remove_punct(x))\n",
    "#Appliyng tokenization\n",
    "def tokenization(text):\n",
    "    text = re.split('\\W+', text)\n",
    "    return text\n",
    "tw_list['tokenized'] = tw_list['punct'].apply(lambda x: tokenization(x.lower()))\n",
    "#Removing stopwords\n",
    "stopword = nltk.corpus.stopwords.words('english')\n",
    "def remove_stopwords(text):\n",
    "    text = [word for word in text if word not in stopword]\n",
    "    return text\n",
    "    \n",
    "tw_list['nonstop'] = tw_list['tokenized'].apply(lambda x: remove_stopwords(x))\n",
    "#Appliyng Stemmer\n",
    "ps = nltk.PorterStemmer()\n",
    "def stemming(text):\n",
    "    text = [ps.stem(word) for word in text]\n",
    "    return text\n",
    "tw_list['stemmed'] = tw_list['nonstop'].apply(lambda x: stemming(x))\n",
    "#Cleaning Text\n",
    "def clean_text(text):\n",
    "    text_lc = \"\".join([word.lower() for word in text if word not in string.punctuation]) # remove puntuation\n",
    "    text_rc = re.sub('[0-9]+', '', text_lc)\n",
    "    tokens = re.split('\\W+', text_rc)    # tokenization\n",
    "    text = [ps.stem(word) for word in tokens if word not in stopword]  # remove stopwords and stemming\n",
    "    return text\n",
    "tw_list.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f98b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying Countvectorizer\n",
    "countVectorizer = CountVectorizer(analyzer=clean_text) \n",
    "countVector = countVectorizer.fit_transform(tw_list['text'])\n",
    "print('{} Number of reviews has {} words'.format(countVector.shape[0], countVector.shape[1]))\n",
    "#print(countVectorizer.get_feature_names())\n",
    "count_vect_df = pd.DataFrame(countVector.toarray(), columns=countVectorizer.vocabulary_.keys())\n",
    "count_vect_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec2d295",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Most Used Words\n",
    "count = pd.DataFrame(count_vect_df.sum())\n",
    "countdf = count.sort_values(0,ascending=False).head(20)\n",
    "countdf[1:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928d0a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to ngram\n",
    "\n",
    "\n",
    "def get_top_n_gram(corpus, ngram_range, n=None):\n",
    "    vec = CountVectorizer(ngram_range=ngram_range,\n",
    "                          stop_words='english').fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0)\n",
    "    words_freq = [(word, sum_words[0, idx])\n",
    "                  for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]\n",
    "\n",
    "\n",
    "# n2_bigram\n",
    "n2_bigrams = get_top_n_gram(tw_list['text'], (2, 2), 20)\n",
    "n2_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a50bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n3_trigram\n",
    "n3_trigrams = get_top_n_gram(tw_list['text'], (3, 3), 20)\n",
    "n3_trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f0813d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# استخراج المستخدمين الذين نشروا هذه التغريدات\n",
    "users = [tweet.user for tweet in tweets]\n",
    "\n",
    "# إنشاء موقع الخادم المحلي لـ Nominatim\n",
    "geolocator = Nominatim(user_agent=\"my_app\")\n",
    "\n",
    "# إنشاء قائمة بالمواقع التي تم تحديدها من قبل هؤلاء المستخدمين\n",
    "locations = [user.location for user in users]\n",
    "\n",
    "# تحويل المواقع إلى إحداثيات\n",
    "geocoded_locations = []\n",
    "for location in locations:\n",
    "    try:\n",
    "        geocoded_location = geolocator.geocode(location, timeout=10)\n",
    "        geocoded_locations.append(geocoded_location)\n",
    "    except:\n",
    "        geocoded_locations.append(None)\n",
    "\n",
    "# تحويل الإحداثيات إلى مدن\n",
    "cities = []\n",
    "for location in geocoded_locations:\n",
    "    if location:\n",
    "        cities.append(location.raw['address']['city'])\n",
    "    else:\n",
    "        cities.append(None)\n",
    "print(cities)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b4fbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets(username):\n",
    "    \"\"\"Returns a list of tweets from a given user\"\"\"\n",
    "    tweets = []\n",
    "    try:\n",
    "        fetched_tweets = api.user_timeline(screen_name=username, count=200, tweet_mode='extended')\n",
    "        tweets.extend(fetched_tweets)\n",
    "        if len(fetched_tweets) > 0:\n",
    "            oldest_tweet = fetched_tweets[-1].id - 1\n",
    "            while len(fetched_tweets) > 0:\n",
    "                fetched_tweets = api.user_timeline(screen_name=username, count=200, max_id=oldest_tweet, tweet_mode='extended')\n",
    "                tweets.extend(fetched_tweets)\n",
    "                if len(fetched_tweets) > 0:\n",
    "                    oldest_tweet = fetched_tweets[-1].id - 1\n",
    "    except tweepy.TweepError as e:\n",
    "        print(f\"Error fetching tweets for {username}: {e}\")\n",
    "    return tweets\n",
    "\n",
    "def get_average_account_age(tweets):\n",
    "    \"\"\"Returns the average age of the Twitter accounts that posted the given tweets\"\"\"\n",
    "    # extract the users who tweeted\n",
    "    users = [tweet.user for tweet in tweets]\n",
    "    # extract the account creation dates for each user\n",
    "    account_creation_dates = [datetime.strptime(user.created_at.strftime('%Y-%m-%d %H:%M:%S'), '%Y-%m-%d %H:%M:%S').date() for user in users if user.created_at is not None]\n",
    "    # calculate the age of each account\n",
    "    ages = [(datetime.now().date() - account_creation_date).days / 365 for account_creation_date in account_creation_dates]\n",
    "    # calculate the average account age\n",
    "    average_age = sum(ages) / len(ages)\n",
    "    return average_age\n",
    "\n",
    "# example usage\n",
    "tweets = get_tweets('example')\n",
    "average_age = get_average_account_age(tweets)\n",
    "print(f\"Average account age of users who tweeted: {average_age:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ee794e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_words(tweets, n):\n",
    "    \"\"\"Returns the n most common words in the given tweets\"\"\"\n",
    "    # join all tweets into one string\n",
    "    tweet_text = ' '.join([tweet.full_text for tweet in tweets])\n",
    "    # remove URLs and mentions\n",
    "    tweet_text = re.sub(r'https?:\\/\\/\\S+|@[\\w_]+', '', tweet_text)\n",
    "    # remove punctuation and convert to lowercase\n",
    "    tweet_text = re.sub(r'[^\\w\\s]','',tweet_text).lower()\n",
    "    # split into words\n",
    "    words = tweet_text.split()\n",
    "    # count word frequency\n",
    "    word_counts = Counter(words)\n",
    "    # return the n most common words\n",
    "    return word_counts.most_common(n)\n",
    "tweets = get_tweets('example')\n",
    "top_words = get_top_words(tweets, 10)\n",
    "print(top_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd84453",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
